# 2025-10-10 - Lektion 18

## Dagordning
* Implementering av dense-lager i mjukvara (del II) - feedforward, backpropagation och optimering.

## Mål med lektionen
* Ha implementerat metoderna `feedforward`, `backpropagate` och `optimize` i dense-lager-klassen.
* Ha skapat en fullt fungerande dense-lager-implementation.
* Ha testat implementationen med ett komplett neuralt nätverk (det som skapades i inlämningsuppgift 5).

## Instruktioner
* Fortsätt där vi arbetade senast - implementera de tre metoderna som lämnades tomma förra lektionen.
* Översätt de matematiska operationerna för feedforward, backpropagation och optimering som vi genomfört för hand till kod.
* Testa implementationen genom att träna det neurala nätverk på befintlig träningsdata, justera vid behov antalet genomförda epoker och/eller lärhastigheten.

## Utvärdering
* Några synpunkter på lektionen i sig?

## Nästa lektion (i januari)
* Lite repetition av neurala nätverk för att förbereda er inför resterande delar av kursen:
    * **Projekt 2** – Implementering av dynamiskt neuralt nätverk på ett inbyggt system.
    * Konvolutionella neurala nätverk för bildklassificering – kernels, padding, pooling-lager med mera.

## Övrigt
* Lycka till på LIA 1, vi ses i januari!
